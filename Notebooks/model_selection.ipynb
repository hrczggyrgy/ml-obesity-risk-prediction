{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"a85d8efdab534637ba6f54fca8a8f101","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"83448c1a317244c48086dcbe0348c230","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":122261,"execution_start":1710684625529,"source_hash":"1cc1d3e1"},"outputs":[{"data":{"text/html":["Finishing last run (ID:39lrfhc5) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">light-elevator-5</strong> at: <a href='https://wandb.ai/herczeg-gyrgy/obesity_prediction_model/runs/39lrfhc5' target=\"_blank\">https://wandb.ai/herczeg-gyrgy/obesity_prediction_model/runs/39lrfhc5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240317_082857-39lrfhc5/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:39lrfhc5). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/datasets/_deepnote_work/wandb/run-20240317_141026-7panysb3</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/herczeg-gyrgy/obesity_prediction_model/runs/7panysb3' target=\"_blank\">iconic-grass-6</a></strong> to <a href='https://wandb.ai/herczeg-gyrgy/obesity_prediction_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/herczeg-gyrgy/obesity_prediction_model' target=\"_blank\">https://wandb.ai/herczeg-gyrgy/obesity_prediction_model</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/herczeg-gyrgy/obesity_prediction_model/runs/7panysb3' target=\"_blank\">https://wandb.ai/herczeg-gyrgy/obesity_prediction_model/runs/7panysb3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Decision Tree - F1 score: 0.8502434520163922\n","Decision Tree - Accuracy: 0.84980323567993\n","Decision Tree - Precision: 0.8509181743895884\n","Decision Tree - Recall: 0.84980323567993\n","/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","\n","Logistic Regression - F1 score: 0.8521645380752952\n","Logistic Regression - Accuracy: 0.8533012680367293\n","Logistic Regression - Precision: 0.8516649925983462\n","Logistic Regression - Recall: 0.8533012680367293\n","\n","K-Nearest Neighbors - F1 score: 0.8352647634436812\n","K-Nearest Neighbors - Accuracy: 0.8358111062527328\n","K-Nearest Neighbors - Precision: 0.8352186253433749\n","K-Nearest Neighbors - Recall: 0.8358111062527328\n","\n","Random Forest - F1 score: 0.9018746642941297\n","Random Forest - Accuracy: 0.9018364669873197\n","Random Forest - Precision: 0.9020273465991757\n","Random Forest - Recall: 0.9018364669873197\n","\n","Gradient Boosting - F1 score: 0.9017161558695133\n","Gradient Boosting - Accuracy: 0.9018364669873197\n","Gradient Boosting - Precision: 0.9017158091713282\n","Gradient Boosting - Recall: 0.9018364669873197\n","\n","SVM - F1 score: 0.49932889894748433\n","SVM - Accuracy: 0.5220813292522956\n","SVM - Precision: 0.5225051743552127\n","SVM - Recall: 0.5220813292522956\n","\n","Neural Network - F1 score: 0.6904115169449768\n","Neural Network - Accuracy: 0.7229995627459554\n","Neural Network - Precision: 0.8057287226328658\n","Neural Network - Recall: 0.7229995627459554\n","\n","Ridge Classifier - F1 score: 0.7112865788881023\n","Ridge Classifier - Accuracy: 0.7275907302142545\n","Ridge Classifier - Precision: 0.7145158153818503\n","Ridge Classifier - Recall: 0.7275907302142545\n","\n","Extra Trees - F1 score: 0.898289573124628\n","Extra Trees - Accuracy: 0.8983384346305203\n","Extra Trees - Precision: 0.8983343603009274\n","Extra Trees - Recall: 0.8983384346305203\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression, RidgeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import (\n","    RandomForestClassifier,\n","    GradientBoostingClassifier,\n","    ExtraTreesClassifier,\n",")\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","import wandb\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","import pandas as pd\n","from feature_engineering import create_features\n","\n","# Loading the data\n","obesity_data = pd.read_csv(\"/work/merged_dataset.csv\")\n","obesity_data = create_features(obesity_data)\n","obesity_data = obesity_data.fillna(obesity_data.median(numeric_only=True))  # Replacing missing values with median for numeric columns only\n","\n","le = LabelEncoder()\n","# Apply LabelEncoder to each column with categorical data separately\n","for col in obesity_data.columns:\n","    if obesity_data[col].dtype == 'object' or obesity_data[col].dtype.name == 'category':\n","        obesity_data[col] = le.fit_transform(obesity_data[col])\n","\n","# Splitting data into features and target variable\n","X = obesity_data.drop([\"NObeyesdad\", \"id\"], axis=1)\n","y = obesity_data[\"NObeyesdad\"]\n","\n","# Splitting the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","wandb.init(project=\"obesity_prediction_model\", entity=\"herczeg-gyrgy\")\n","\n","# Decision Tree Classifier\n","dtree = DecisionTreeClassifier(random_state=42)\n","dtree.fit(X_train, y_train)\n","y_pred = dtree.predict(X_test)\n","# Evaluating performance\n","print(\"Decision Tree - F1 score:\", f1_score(y_test, y_pred, average=\"weighted\"))\n","print(\"Decision Tree - Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"Decision Tree - Precision:\", precision_score(y_test, y_pred, average=\"weighted\"))\n","print(\"Decision Tree - Recall:\", recall_score(y_test, y_pred, average=\"weighted\"))\n","\n","# Other classifiers implementations\n","classifiers = {\n","    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=5000, n_jobs=-1),\n","    \"K-Nearest Neighbors\": KNeighborsClassifier(n_jobs=-1),\n","    \"Random Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n","    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n","    \"SVM\": SVC(random_state=42),\n","    \"Neural Network\": MLPClassifier(random_state=42, max_iter=5000),\n","    \"Ridge Classifier\": RidgeClassifier(random_state=42),\n","    \"Extra Trees\": ExtraTreesClassifier(random_state=42, n_jobs=-1),\n","}\n","\n","metrics = []\n","\n","for name, classifier in classifiers.items():\n","    classifier.fit(X_train, y_train)\n","    y_pred = classifier.predict(X_test)\n","    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average=\"weighted\")\n","    recall = recall_score(y_test, y_pred, average=\"weighted\")\n","    print(f\"\\n{name} - F1 score:\", f1)\n","    print(f\"{name} - Accuracy:\", accuracy)\n","    print(f\"{name} - Precision:\", precision)\n","    print(f\"{name} - Recall:\", recall)\n","    metrics.append({\n","        \"model\": name,\n","        \"F1 score\": f1,\n","        \"Accuracy\": accuracy,\n","        \"Precision\": precision,\n","        \"Recall\": recall\n","    })\n","\n","wandb.log({\"metrics\": metrics})\n","\n","for metric in metrics:\n","    # Feature importance for tree-based classifiers\n","    if hasattr(classifiers[metric[\"model\"]], \"feature_importances_\"):\n","        feature_importance = classifiers[metric[\"model\"]].feature_importances_\n","        feature_data = pd.DataFrame({\n","            \"Feature\": X_train.columns,\n","            \"Importance\": feature_importance\n","        })\n","        wandb_table = wandb.Table(dataframe=feature_data)\n","        wandb.log({f\"{metric['model']} Feature Importance\": wandb.plot.bar(wandb_table, \"Feature\", \"Importance\", title=f\"{metric['model']} Feature Importance\")})"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"3c19137a29de4256949f5ffa91d35c35","language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
